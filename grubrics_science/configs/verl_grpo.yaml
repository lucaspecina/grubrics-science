# =============================================================================
# veRL GRPO Production Configuration â€” Azure H100 NVL (94GB)
# =============================================================================
#
# UNIFIED PIPELINE: same veRL framework as debug, different params.
#
# Target: 1x H100 NVL 94GB (Azure Standard_NC40ads_H100_v5)
# Model: Qwen3-8B + LoRA (rank 64)
# Algorithm: GRPO (Group Relative Policy Optimization)
#
# Debug config: verl_grpo_debug.yaml (Workstation RTX 4000, Qwen2.5-0.5B, HF engine)
#
# Usage:
#   python -m verl.trainer.main_ppo \
#     --config grubrics_science/configs/verl_grpo.yaml
#
# Estimated VRAM: ~68GB (fits in 94GB with ~26GB margin)
# =============================================================================

# --- Model ---
model:
  name: "Qwen/Qwen3-8B"
  dtype: "bfloat16"
  trust_remote_code: true

# --- LoRA ---
peft:
  enabled: true
  lora_rank: 64
  lora_alpha: 128        # alpha = 2 * rank is common
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# --- GRPO Algorithm ---
algorithm:
  name: "grpo"
  group_size: 6           # n=6 rollouts per prompt
  kl_coef: 0.01           # KL penalty coefficient
  clip_range: 0.2         # PPO-style clipping
  entropy_coef: 0.01      # encourage exploration early
  max_grad_norm: 1.0

# --- Rollout (vLLM) ---
rollout:
  engine: "vllm"
  max_new_tokens: 512
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  gpu_memory_utilization: 0.85

# --- Training ---
training:
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler: "cosine"
  gradient_checkpointing: true
  gradient_accumulation_steps: 2
  per_device_batch_size: 2  # effective batch = 2 * 2 = 4 prompts * 6 rollouts = 24 samples
  max_steps: 2000
  save_steps: 200
  eval_steps: 100
  logging_steps: 10

# --- Data ---
data:
  train_files:
    - "data/processed/gsm8k_train.parquet"
  max_prompt_length: 2048
  max_response_length: 512

# --- Reward ---
reward:
  # The reward function is registered separately in the training script.
  # For Phase 0: uses gsm8k_reward.compute_score (local, no API)
  # For Phase 1+: uses grubrics_reward.compute_score (routes by data_source)
  reward_module: "grubrics_science.rewards.gsm8k_reward"
  reward_function: "compute_score"

# --- Logging ---
logging:
  project: "grubrics-transfer"
  run_name: "phase0-gsm8k-grpo"
  use_wandb: true

# --- Checkpointing ---
checkpoint:
  output_dir: "checkpoints/grubrics-transfer"
  save_total_limit: 5
