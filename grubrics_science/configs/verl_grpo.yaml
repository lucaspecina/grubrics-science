# =============================================================================
# veRL GRPO Production Configuration — Azure H100 NVL (94GB)
# =============================================================================
#
# UNIFIED PIPELINE: same veRL framework as debug, different params.
#
# Target: 1x H100 NVL 94GB (Azure Standard_NC40ads_H100_v5)
# Model: Qwen3-8B + LoRA (rank 64)
# Algorithm: GRPO (Group Relative Policy Optimization)
#
# Debug config: verl_grpo_debug.yaml (Workstation RTX 4000, Qwen2.5-0.5B, HF engine)
#
# Usage:
#   python run_grpo.py --config grubrics_science/configs/verl_grpo.yaml
#
# Estimated VRAM: ~68GB (fits in 94GB with ~26GB margin)
# =============================================================================

# --- Model + Actor + Rollout ---
actor_rollout_ref:
  model:
    path: Qwen/Qwen3-8B
    trust_remote_code: true
    enable_gradient_checkpointing: true
    use_remove_padding: true
    # LoRA config
    lora_rank: 64
    lora_alpha: 128
    target_modules: all-linear

  actor:
    strategy: fsdp
    ppo_mini_batch_size: 64
    ppo_micro_batch_size_per_gpu: 4
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.01
    use_kl_loss: false
    grad_clip: 1.0
    use_torch_compile: true
    optim:
      lr: 1.0e-5
      lr_warmup_steps_ratio: 0.1
      weight_decay: 0.01
      lr_scheduler_type: cosine

  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1
    top_p: 0.95
    do_sample: true
    n: 6
    gpu_memory_utilization: 0.85
    tensor_model_parallel_size: 1
    enforce_eager: false
    log_prob_micro_batch_size_per_gpu: 4

  hybrid_engine: true

# --- GRPO Algorithm ---
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  kl_ctrl:
    type: fixed
    kl_coef: 0.01

# --- Data ---
data:
  train_files: data/processed/gsm8k_train.parquet
  val_files: data/processed/gsm8k_train.parquet
  max_prompt_length: 2048
  max_response_length: 512
  train_batch_size: 24
  shuffle: true
  trust_remote_code: true
  filter_overlong_prompts: true
  truncation: left

# --- Custom Reward Function ---
custom_reward_function:
  path: grubrics_science/rewards/gsm8k_reward.py
  name: compute_score

# --- Reward Model (disabled — we use custom_reward_function) ---
reward_model:
  enable: false

# --- Critic (disabled for GRPO) ---
critic:
  enable: false

# --- Trainer ---
trainer:
  total_training_steps: 2000
  project_name: grubrics-transfer
  experiment_name: phase0-gsm8k-grpo
  logger:
    - console
    - wandb
  n_gpus_per_node: 1
  nnodes: 1
  save_freq: 200
  test_freq: 100
  val_before_train: true
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  device: cuda
