# =============================================================================
# veRL GRPO Debug Configuration â€” Workstation RTX 4000 Ada (12GB)
# =============================================================================
#
# UNIFIED PIPELINE: same veRL framework as production, different params.
#
# Target: Workstation RTX 4000 Ada (12GB VRAM, compute capability 8.9)
# Model: Qwen2.5-0.5B-Instruct (instead of Qwen3-8B)
# Purpose: Debug full veRL GRPO pipeline before running on H100
#
# Production config: verl_grpo.yaml (H100, Qwen3-8B, vLLM, rank 64)
#
# Differences vs production:
#   - Model: 0.5B instead of 8B (~1GB VRAM vs ~16GB)
#   - Rollout engine: HF generate instead of vLLM (fits 12GB)
#   - LoRA rank: 16 instead of 64
#   - group_size: 2 instead of 6
#   - max_new_tokens: 256 instead of 512
#   - max_steps: 20 instead of 2000
#   - Dataset truncated to 50 items
#   - No wandb logging
#
# What's shared: veRL framework, data format, reward function, config structure
#
# Usage:
#   python -m verl.trainer.main_ppo --config grubrics_science/configs/verl_grpo_debug.yaml
#
# Estimated VRAM: ~3.5-5.5 GB (fits comfortably in 12GB)
# =============================================================================

# --- Model ---
model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  dtype: "float16"          # fp16 for wider compatibility
  trust_remote_code: true

# --- LoRA ---
peft:
  enabled: true
  lora_rank: 16             # smaller rank for debug
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# --- GRPO Algorithm ---
algorithm:
  name: "grpo"
  group_size: 2             # minimal rollouts for debug
  kl_coef: 0.01
  clip_range: 0.2
  entropy_coef: 0.01
  max_grad_norm: 1.0

# --- Rollout ---
rollout:
  engine: "hf"              # HuggingFace generate (no vLLM for local)
  max_new_tokens: 256       # shorter for speed
  temperature: 1.0
  top_k: 50
  top_p: 0.95

# --- Training ---
training:
  learning_rate: 5.0e-5     # slightly higher for small model
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler: "cosine"
  gradient_checkpointing: false  # not needed for 0.5B
  gradient_accumulation_steps: 1
  per_device_batch_size: 1
  max_steps: 20             # just enough to verify gradients flow
  save_steps: 10
  eval_steps: 10
  logging_steps: 1

# --- Data ---
data:
  train_files:
    - "data/processed/test/olympiad_math_test.parquet"
  max_prompt_length: 1024
  max_response_length: 256
  max_items: 50             # truncate dataset for debug

# --- Reward ---
reward:
  reward_module: "grubrics_science.rewards.gsm8k_reward"
  reward_function: "compute_score"

# --- Logging ---
logging:
  project: "grubrics-transfer-debug"
  run_name: "debug-local"
  use_wandb: false          # no wandb for local debug

# --- Checkpointing ---
checkpoint:
  output_dir: "checkpoints/debug"
  save_total_limit: 2
