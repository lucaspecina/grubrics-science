# =============================================================================
# veRL GRPO Debug Configuration — Small GPU (Linux, ~12GB+ VRAM)
# =============================================================================
#
# UNIFIED PIPELINE: same veRL framework as production, smaller params.
#
# Target: Any Linux GPU with 12GB+ VRAM (e.g. RTX 4000 Ada, A10, small Azure VM)
# Model: Qwen2.5-0.5B-Instruct (instead of Qwen3-8B)
# Purpose: Debug full veRL GRPO pipeline before running on H100
#
# Production config: configs/verl_grpo.yaml (H100, Qwen3-8B, vLLM, rank 64)
#
# Differences vs production:
#   - Model: 0.5B instead of 8B (~1GB VRAM vs ~16GB)
#   - LoRA rank: 16 instead of 64
#   - group_size (rollout.n): 2 instead of 6
#   - max_response_length: 256 instead of 512
#   - total_training_steps: 20 instead of 2000
#   - No wandb logging
#
# Usage:
#   python run_grpo.py --config configs/verl_grpo_debug.yaml
#
# Estimated VRAM: ~3.5-5.5 GB (fits comfortably in 12GB)
# =============================================================================

# --- Model + Actor + Rollout ---
actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    trust_remote_code: true
    enable_gradient_checkpointing: false
    use_remove_padding: false
    # LoRA config (smaller rank for debug)
    lora_rank: 16
    lora_alpha: 32
    target_modules: all-linear

  actor:
    strategy: fsdp
    ppo_mini_batch_size: 2
    ppo_micro_batch_size_per_gpu: 2
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.01
    use_kl_loss: false
    grad_clip: 1.0
    use_torch_compile: false
    optim:
      lr: 5.0e-5
      lr_warmup_steps_ratio: 0.1
      weight_decay: 0.01
      lr_scheduler_type: cosine

  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1
    top_p: 0.95
    do_sample: true
    n: 2
    gpu_memory_utilization: 0.4
    tensor_model_parallel_size: 1
    enforce_eager: true
    log_prob_micro_batch_size_per_gpu: 2

  hybrid_engine: true

# --- GRPO Algorithm ---
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  kl_ctrl:
    type: fixed
    kl_coef: 0.01

# --- Data ---
# Generated via: python -m grubrics_science.data.prepare preset --output_dir data/processed
# Active preset: open_only (HealthBench). See training_presets.yaml for options.
data:
  train_files: data/processed/mixed_train.parquet
  val_files: data/processed/mixed_train.parquet
  max_prompt_length: 1024
  max_response_length: 256
  train_batch_size: 4
  shuffle: true
  trust_remote_code: true
  filter_overlong_prompts: true
  truncation: left
  dataloader_num_workers: 0
  filter_overlong_prompts_workers: 1

# --- Custom Reward Function ---
# grubrics_reward.py routes by data_source: verifiable → local, open → Judge API
custom_reward_function:
  path: grubrics_science/rewards/grubrics_reward.py
  name: compute_score

# --- Reward Weights (Phase 3 configurable) ---
reward_config:
  lambda_len: 0.1
  lambda_info: 0.3
  lambda_defense: 0.3
  char_threshold: 3000
  use_contrastive: true

# --- Reward Model (disabled — we use custom_reward_function) ---
reward_model:
  enable: false

# --- Critic (disabled for GRPO) ---
critic:
  enable: false

# --- Trainer ---
trainer:
  total_training_steps: 20
  project_name: grubrics-transfer-debug
  experiment_name: debug-local
  logger:
    - console
  n_gpus_per_node: 1
  nnodes: 1
  save_freq: 10
  test_freq: 10
  val_before_train: false
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  device: cuda
