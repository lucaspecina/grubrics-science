# =============================================================================
# veRL GRPO Production Configuration — Azure H100 NVL (94GB)
# =============================================================================
#
# UNIFIED PIPELINE: same veRL framework as debug, different params.
#
# Target: 1x H100 NVL 94GB (Azure Standard_NC40ads_H100_v5)
# Model: Qwen3-8B + LoRA (rank 64)
# Algorithm: GRPO (Group Relative Policy Optimization)
#
# Debug config: configs/verl_grpo_debug.yaml (Workstation RTX 4000, Qwen2.5-0.5B, HF engine)
#
# Usage:
#   python run_grpo.py --config configs/verl_grpo.yaml
#
# Estimated VRAM: ~80GB (FSDP actor ~33GB + vLLM rollout ~47GB, fits in 94GB)
# =============================================================================

# --- Model + Actor + Rollout ---
actor_rollout_ref:
  model:
    path: Qwen/Qwen3-8B
    trust_remote_code: true
    enable_gradient_checkpointing: true
    use_remove_padding: true
    # LoRA config
    lora_rank: 64
    lora_alpha: 128
    target_modules: all-linear

  actor:
    strategy: fsdp
    ppo_mini_batch_size: 64
    ppo_micro_batch_size_per_gpu: 4
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.01
    use_kl_loss: false
    grad_clip: 1.0
    use_torch_compile: true
    optim:
      lr: 1.0e-5
      lr_warmup_steps_ratio: 0.1
      weight_decay: 0.01
      lr_scheduler_type: cosine

  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1
    top_p: 0.95
    do_sample: true
    n: 6
    gpu_memory_utilization: 0.5      # hybrid engine shares GPU with FSDP actor (~33GB)
    tensor_model_parallel_size: 1
    enforce_eager: false
    log_prob_micro_batch_size_per_gpu: 4

  hybrid_engine: true

# --- GRPO Algorithm ---
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  kl_ctrl:
    type: fixed
    kl_coef: 0.01

# --- Data ---
# Generated via: python -m grubrics_science.data.prepare preset --output_dir data/processed
# Active preset: open_only (HealthBench). See training_presets.yaml for options.
data:
  train_files: data/processed/mixed_train.parquet
  val_files: data/processed/mixed_train.parquet
  max_prompt_length: 2048
  max_response_length: 512
  train_batch_size: 24
  shuffle: true
  trust_remote_code: true
  filter_overlong_prompts: true
  truncation: left
  # Reduce DataLoader workers to lower RAM (avoids OOM at end of training)
  num_workers: 2

# --- Custom Reward Function ---
# grubrics_reward.py routes by data_source: verifiable → local, open → Judge API
custom_reward_function:
  path: grubrics_science/rewards/grubrics_reward.py
  name: compute_score

# --- Reward Weights (Phase 3 configurable) ---
# These are passed as env vars to the reward function.
# To disable a component for ablation, set its lambda to 0.0.
# To disable functional alignment entirely (B4 ablation), set use_functional to false.
reward_config:
  lambda_len: 0.1          # Length penalty weight
  lambda_info: 0.3         # Info value bonus weight
  lambda_defense: 0.3      # Defense penalty weight
  char_threshold: 3000     # Chars before length penalty kicks in
  use_contrastive: true    # false = no contrastive excerpts in prompt (A1 ablation)

# --- Reward Model (disabled — we use custom_reward_function) ---
reward_model:
  enable: false

# --- Critic (disabled for GRPO) ---
critic:
  enable: false

# --- Trainer ---
trainer:
  total_training_steps: 2000
  project_name: grubrics-transfer
  experiment_name: healthbench-grpo
  logger:
    - console
    - wandb
  n_gpus_per_node: 1
  nnodes: 1
  save_freq: 200
  test_freq: 100
  val_before_train: true
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  device: cuda
