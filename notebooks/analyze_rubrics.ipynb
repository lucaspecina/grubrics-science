{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GRubrics — Análisis de Rúbricas Generadas\n\nNotebook para inspeccionar y comparar rúbricas generadas por distintos modelos/checkpoints.\n\n**Cómo usar:**\n1. Correr las celdas de Setup (§1-§4) una sola vez por sesión.\n2. En §5, cargar los checkpoints que querés comparar (tarda 1-2 min cada uno).\n3. Navegar preguntas del holdout en §6 y generar rúbricas en §7.\n4. Comparar múltiples checkpoints para la misma pregunta en §8.\n5. Evaluación con Judge (opcional, ~$0.009/pregunta) en §9.\n6. Mini-eval sobre N preguntas aleatorias en §10.\n7. Comparar métricas entre checkpoints (CSVs de §10) en §11.\n8. **Inspeccionar rúbricas guardadas durante el training en §12** (sin recargar modelos).\n\n**Rúbricas durante training:** La reward function guarda automáticamente el texto de cada\nrúbrica generada en `data/results/rubrics/step_XXXX.jsonl`. §12 las carga y grafica cómo\nevolucionan alignment, reward y largo a lo largo del training.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §1 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Project root (notebook está en notebooks/, proyecto en ..)\n",
    "ROOT = Path(\".\").resolve().parent\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from grubrics_science.data.base import DatasetAdapter\n",
    "from grubrics_science.evaluation.holdout import load_healthbench_with_cache, split_holdout\n",
    "from grubrics_science.evaluation.metrics import (\n",
    "    alignment_score, discrimination_score, format_validity,\n",
    "    points_sum, info_value, rubric_length,\n",
    ")\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()} | device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §2 — Configuración de paths y checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Paths ────────────────────────────────────────────────────────────────────\n",
    "CHECKPOINT_DIR  = ROOT / \"checkpoints\" / \"grubrics-transfer\" / \"healthbench-grpo\"\n",
    "SFT_DIR         = ROOT / \"checkpoints\" / \"grubrics-transfer\" / \"sft-healthbench\" / \"final\"\n",
    "HB_EVAL_PATH    = ROOT / \"data\" / \"healthbench\" / \"oss_eval.jsonl\"\n",
    "HB_CACHE_PATH   = ROOT / \"data\" / \"cache\" / \"healthbench_precompute.jsonl\"\n",
    "BASE_MODEL_ID   = \"Qwen/Qwen3-8B\"\n",
    "HOLDOUT_SIZE    = 500\n",
    "HOLDOUT_SEED    = 42\n",
    "\n",
    "# ─── Detectar checkpoints disponibles automáticamente ─────────────────────────\n",
    "checkpoints = {}\n",
    "\n",
    "# Base model (sin fine-tuning)\n",
    "checkpoints[\"base_zeroshot\"] = BASE_MODEL_ID\n",
    "\n",
    "# SFT\n",
    "if SFT_DIR.exists():\n",
    "    checkpoints[\"sft\"] = str(SFT_DIR)\n",
    "\n",
    "# GRPO steps (step_200/actor, step_400/actor, ...)\n",
    "if CHECKPOINT_DIR.exists():\n",
    "    for actor_dir in sorted(CHECKPOINT_DIR.glob(\"step_*/actor\"),\n",
    "                            key=lambda p: int(p.parent.name.split(\"_\")[1])):\n",
    "        step = int(actor_dir.parent.name.split(\"_\")[1])\n",
    "        checkpoints[f\"grpo_step{step}\"] = str(actor_dir)\n",
    "\n",
    "print(\"Checkpoints detectados:\")\n",
    "for name, path in checkpoints.items():\n",
    "    exists = \"✓\" if (path == BASE_MODEL_ID or Path(path).exists()) else \"✗ (no encontrado)\"\n",
    "    print(f\"  [{exists}] {name:25s} → {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §3 — Cargar holdout HealthBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_data = load_healthbench_with_cache(\n",
    "    eval_path=str(HB_EVAL_PATH),\n",
    "    cache_path=str(HB_CACHE_PATH),\n",
    ")\n",
    "_, holdout = split_holdout(hb_data, holdout_size=HOLDOUT_SIZE, seed=HOLDOUT_SEED)\n",
    "\n",
    "# Solo quedarse con los que tienen gold_scores (precompute completo)\n",
    "holdout_with_scores = [e for e in holdout if e.get(\"gold_scores\")]\n",
    "\n",
    "print(f\"Holdout total         : {len(holdout)} preguntas\")\n",
    "print(f\"Con gold_scores       : {len(holdout_with_scores)} preguntas\")\n",
    "print(f\"Sin gold_scores (skip): {len(holdout) - len(holdout_with_scores)} preguntas\")\n",
    "\n",
    "# Vista rápida de la primera\n",
    "q = holdout_with_scores[0]\n",
    "print(f\"\\nEjemplo [0]:\")\n",
    "print(f\"  ID       : {q.get('question_id', q.get('prompt_id', '?'))}\")\n",
    "print(f\"  Pregunta : {q['question'][:150]}...\")\n",
    "print(f\"  Answers  : {len(q['answers'])}\")\n",
    "print(f\"  Gold     : {[round(s, 2) for s in q['gold_scores']]}\")\n",
    "print(f\"  Golden rubric (primeros 200 chars): {q['golden_rubric'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §4 — Helpers: carga de modelos y generación de rúbricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "_model_cache: dict = {}  # nombre → (model, tokenizer)\n",
    "\n",
    "\n",
    "def load_checkpoint(name: str):\n",
    "    \"\"\"Carga base Qwen3-8B + LoRA adapter (si aplica). Usa caché para no recargar.\n",
    "    \n",
    "    veRL guarda el actor en HF format. Intentamos cargar como LoRA adapter (peft).\n",
    "    Si falla (ej. pesos mergeados), cargamos directo como HF model.\n",
    "    \"\"\"\n",
    "    if name in _model_cache:\n",
    "        print(f\"✓ {name!r} ya está en caché\")\n",
    "        return _model_cache[name]\n",
    "\n",
    "    path = checkpoints.get(name)\n",
    "    if path is None:\n",
    "        raise ValueError(f\"Checkpoint {name!r} no encontrado. Disponibles: {list(checkpoints)}\")\n",
    "\n",
    "    is_base = (path == BASE_MODEL_ID)\n",
    "    print(f\"Cargando {name!r}...\")\n",
    "    print(f\"  Base model : {BASE_MODEL_ID}\")\n",
    "    if not is_base:\n",
    "        print(f\"  Adapter    : {path}\")\n",
    "\n",
    "    # Tokenizer: intentar desde el checkpoint, fallback al base model\n",
    "    tok_path = BASE_MODEL_ID if is_base else path\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tok_path, trust_remote_code=True)\n",
    "    except Exception:\n",
    "        print(f\"  ⚠ Tokenizer no en {path}, usando base model\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "    # Base model\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if is_base:\n",
    "        model = base\n",
    "    else:\n",
    "        # Intentar cargar como LoRA adapter (peft format — lo que guarda veRL y TRL)\n",
    "        adapter_config = Path(path) / \"adapter_config.json\"\n",
    "        if adapter_config.exists():\n",
    "            from peft import PeftModel\n",
    "            print(\"  → Cargando como peft LoRA adapter\")\n",
    "            model = PeftModel.from_pretrained(base, path)\n",
    "            model = model.merge_and_unload()  # merge para inferencia más rápida\n",
    "        else:\n",
    "            # Checkpoint con pesos mergeados (HF completo)\n",
    "            print(\"  → No hay adapter_config.json, cargando como HF model completo\")\n",
    "            del base\n",
    "            torch.cuda.empty_cache()\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    _model_cache[name] = (model, tokenizer)\n",
    "    print(f\"  ✓ Listo — {sum(p.numel() for p in model.parameters())/1e9:.1f}B params en {next(model.parameters()).device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def unload_checkpoint(name: str):\n",
    "    \"\"\"Libera VRAM de un checkpoint.\"\"\"\n",
    "    if name in _model_cache:\n",
    "        del _model_cache[name]\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✓ {name!r} descargado de VRAM\")\n",
    "\n",
    "\n",
    "def unload_all():\n",
    "    \"\"\"Libera todos los modelos cargados.\"\"\"\n",
    "    for name in list(_model_cache.keys()):\n",
    "        unload_checkpoint(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HB_CONTEXT = (\n",
    "    \"This is a medical conversation between a patient and an AI assistant. \"\n",
    "    \"The rubric should evaluate medical accuracy, completeness, safety, \"\n",
    "    \"communication quality, and instruction following.\"\n",
    ")\n",
    "\n",
    "\n",
    "def _build_messages(entry: dict, use_contrastive: bool = True) -> list:\n",
    "    \"\"\"Construye los mensajes de chat exactamente igual que el HealthBench adapter.\"\"\"\n",
    "    best_excerpt = worst_excerpt = None\n",
    "    if use_contrastive and entry.get(\"answers\") and entry.get(\"gold_scores\"):\n",
    "        answers = entry[\"answers\"]\n",
    "        scores  = entry[\"gold_scores\"]\n",
    "        if len(answers) > 1:\n",
    "            best_idx  = int(np.argmax(scores))\n",
    "            worst_idx = int(np.argmin(scores))\n",
    "            if best_idx != worst_idx:\n",
    "                best_excerpt  = answers[best_idx][:400]\n",
    "                worst_excerpt = answers[worst_idx][:400]\n",
    "\n",
    "    return DatasetAdapter.build_rubric_generation_prompt(\n",
    "        question=entry[\"question\"],\n",
    "        context=HB_CONTEXT,\n",
    "        best_answer_excerpt=best_excerpt,\n",
    "        worst_answer_excerpt=worst_excerpt,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_rubric(\n",
    "    checkpoint_name: str,\n",
    "    entry: dict,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    "    use_contrastive: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Genera una rúbrica para una pregunta del holdout.\"\"\"\n",
    "    model, tokenizer = load_checkpoint(checkpoint_name)\n",
    "    messages = _build_messages(entry, use_contrastive=use_contrastive)\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "        do_sample=temperature > 0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_ids = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def quick_metrics(rubric: str) -> dict:\n",
    "    \"\"\"Métricas que no requieren Judge API (gratis, instantáneo).\"\"\"\n",
    "    return {\n",
    "        \"format_validity\" : round(format_validity(rubric), 3),\n",
    "        \"points_sum\"      : round(points_sum(rubric), 2),\n",
    "        \"n_items\"         : sum(1 for l in rubric.splitlines() if l.strip().startswith(\"Points:\")),\n",
    "        \"chars\"           : rubric_length(rubric),\n",
    "    }\n",
    "\n",
    "\n",
    "def show_rubric_comparison(rubrics: dict):\n",
    "    \"\"\"Muestra rúbricas side-by-side con métricas rápidas.\"\"\"\n",
    "    rows = []\n",
    "    for name, rubric in rubrics.items():\n",
    "        m = quick_metrics(rubric)\n",
    "        rows.append({\"model\": name, **m})\n",
    "        print(f\"\\n{'═' * 65}\")\n",
    "        print(f\"  {name.upper()}\")\n",
    "        fmt = m['format_validity']\n",
    "        pts = m['points_sum']\n",
    "        print(f\"  format={fmt} | points_sum={pts} | items={m['n_items']} | chars={m['chars']}\")\n",
    "        print(f\"{'─' * 65}\")\n",
    "        print(rubric)\n",
    "\n",
    "    print(f\"\\n{'═' * 65}\")\n",
    "    print(\"RESUMEN MÉTRICAS RÁPIDAS:\")\n",
    "    display(pd.DataFrame(rows).set_index(\"model\"))\n",
    "\n",
    "\n",
    "print(\"Helpers cargados ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §5 — Cargar checkpoints\n",
    "\n",
    "Cargar uno a la vez. El H100 tiene 94GB VRAM — Qwen3-8B ocupa ~16GB en bfloat16,\n",
    "así que podés tener 2-3 modelos cargados simultáneamente si querés comparar rápido.\n",
    "Usá `unload_checkpoint(name)` para liberar VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cargar checkpoints que querés comparar ───────────────────────────────────\n",
    "# Descomentá los que necesitás. El primero que cargues tarda más (descarga base model).\n",
    "\n",
    "load_checkpoint(\"base_zeroshot\")\n",
    "# load_checkpoint(\"sft\")\n",
    "# load_checkpoint(\"grpo_step200\")\n",
    "# load_checkpoint(\"grpo_step400\")\n",
    "\n",
    "print(f\"\\nModelos en caché: {list(_model_cache.keys())}\")\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved  = torch.cuda.memory_reserved() / 1e9\n",
    "    total     = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"VRAM: {allocated:.1f}GB allocada | {reserved:.1f}GB reservada | {total:.1f}GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §6 — Browser del holdout\n",
    "\n",
    "Explorar preguntas. Cambiá `IDX` para navegar. Las que no tienen `gold_scores` fueron excluidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 0  # ← cambiar para explorar (0 a len(holdout_with_scores)-1)\n",
    "\n",
    "entry = holdout_with_scores[IDX]\n",
    "\n",
    "print(f\"[{IDX}/{len(holdout_with_scores)-1}] ID: {entry.get('question_id', entry.get('prompt_id', '?'))}\")\n",
    "print(f\"{'─'*70}\")\n",
    "print(\"PREGUNTA:\")\n",
    "print(entry[\"question\"])\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(\"RUBRICA GOLDEN (referencia humana):\")\n",
    "print(entry[\"golden_rubric\"])\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(f\"Answers disponibles: {len(entry['answers'])}\")\n",
    "print(f\"Gold scores        : {[round(s,3) for s in entry['gold_scores']]}\")\n",
    "m = quick_metrics(entry[\"golden_rubric\"])\n",
    "print(f\"Golden — format={m['format_validity']} | points_sum={m['points_sum']} | items={m['n_items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §7 — Generar rúbrica con un checkpoint\n",
    "\n",
    "Genera la rúbrica para la pregunta seleccionada arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"base_zeroshot\"   # ← cambiar al que querés usar\n",
    "TEMPERATURE = 0.7               # 0.0 = greedy, 0.7 = sampling (como en training)\n",
    "USE_CONTRASTIVE = True          # Incluir excerpts de mejor/peor respuesta (igual que training)\n",
    "\n",
    "print(f\"Generando rúbrica con {CHECKPOINT!r} (temp={TEMPERATURE})...\")\n",
    "generated_rubric = generate_rubric(\n",
    "    CHECKPOINT, entry,\n",
    "    temperature=TEMPERATURE,\n",
    "    use_contrastive=USE_CONTRASTIVE,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'═'*65}\")\n",
    "print(f\"  GENERADA ({CHECKPOINT})\")\n",
    "print(f\"{'─'*65}\")\n",
    "print(generated_rubric)\n",
    "\n",
    "m = quick_metrics(generated_rubric)\n",
    "print(f\"\\nMétricas: format={m['format_validity']} | points_sum={m['points_sum']} | items={m['n_items']} | chars={m['chars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §8 — Comparar múltiples checkpoints\n",
    "\n",
    "Genera la misma pregunta con todos los modelos cargados y compara side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a comparar (solo los que están en caché)\n",
    "MODELS_TO_COMPARE = list(_model_cache.keys())  # todos los cargados\n",
    "# O especificar manualmente:\n",
    "# MODELS_TO_COMPARE = [\"base_zeroshot\", \"sft\", \"grpo_step200\"]\n",
    "\n",
    "QUESTION_IDX = IDX   # usar la pregunta del §6 (o cambiar)\n",
    "entry_cmp = holdout_with_scores[QUESTION_IDX]\n",
    "\n",
    "rubrics = {\"golden\": entry_cmp[\"golden_rubric\"]}\n",
    "for name in MODELS_TO_COMPARE:\n",
    "    print(f\"Generando con {name}...\")\n",
    "    rubrics[name] = generate_rubric(name, entry_cmp, temperature=TEMPERATURE)\n",
    "\n",
    "print(f\"\\nPREGUNTA: {entry_cmp['question'][:150]}...\\n\")\n",
    "show_rubric_comparison(rubrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §9 — Evaluación con Judge (opcional)\n",
    "\n",
    "Llama al Judge API para evaluar las rúbricas contra las respuestas precomputadas.\n",
    "Computa alignment (Spearman) vs gold_scores.\n",
    "\n",
    "**Costo**: ~$0.009 por pregunta × número de modelos.\n",
    "Con 4 modelos y 1 pregunta: ~$0.036."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grubrics_science.judge.judge import Judge\n",
    "\n",
    "JUDGE_MODEL = os.environ.get(\"JUDGE_MODEL\", \"gpt-4.1\")\n",
    "print(f\"Judge model: {JUDGE_MODEL}\")\n",
    "print(f\"Asegurate de tener AZURE_API_KEY y AZURE_API_BASE en el entorno (o en .env)\")\n",
    "\n",
    "\n",
    "async def eval_rubrics_with_judge(entry: dict, rubrics: dict) -> pd.DataFrame:\n",
    "    \"\"\"Evalúa un dict {nombre: rubrica} sobre una pregunta con el Judge.\n",
    "    Devuelve DataFrame con alignment, discrimination, info_value y scores raw.\n",
    "    \"\"\"\n",
    "    judge = Judge(model=JUDGE_MODEL, max_concurrent=10, max_cache_size=0)\n",
    "    rows = []\n",
    "    for name, rubric in rubrics.items():\n",
    "        print(f\"  Evaluando {name}...\")\n",
    "        judge_scores = await judge.evaluate_answers_batched(\n",
    "            question=entry[\"question\"],\n",
    "            answers=entry[\"answers\"],\n",
    "            rubric=rubric,\n",
    "        )\n",
    "        gold = entry[\"gold_scores\"]\n",
    "        rows.append({\n",
    "            \"model\"         : name,\n",
    "            \"alignment\"     : round(alignment_score(judge_scores, gold), 3),\n",
    "            \"discrimination\": round(discrimination_score(judge_scores), 3),\n",
    "            \"info_value\"    : round(info_value(judge_scores), 3),\n",
    "            \"format\"        : round(format_validity(rubric), 3),\n",
    "            \"points_sum\"    : round(points_sum(rubric), 2),\n",
    "            \"judge_scores\"  : [round(s, 3) for s in judge_scores],\n",
    "            \"gold_scores\"   : [round(s, 3) for s in gold],\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"model\")\n",
    "\n",
    "\n",
    "# Jupyter soporta await directo en celdas (IPython 7+)\n",
    "print(\"\\nEvaluando con Judge...\")\n",
    "df_judge = await eval_rubrics_with_judge(entry_cmp, rubrics)\n",
    "\n",
    "print(\"\\nResultados:\")\n",
    "display(df_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Visualizar scores raw vs gold scores ─────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, len(rubrics), figsize=(4 * len(rubrics), 4), sharey=True)\n",
    "if len(rubrics) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "gold = entry_cmp[\"gold_scores\"]\n",
    "x = range(len(gold))\n",
    "\n",
    "for ax, (name, _) in zip(axes, rubrics.items()):\n",
    "    if name in df_judge.index:\n",
    "        js = df_judge.loc[name, \"judge_scores\"]\n",
    "        ax.bar(x, gold, alpha=0.4, label=\"gold\", color=\"green\")\n",
    "        ax.bar(x, js,   alpha=0.6, label=\"judge\", color=\"blue\")\n",
    "        corr = df_judge.loc[name, \"alignment\"]\n",
    "        ax.set_title(f\"{name}\\nalignment={corr}\")\n",
    "        ax.set_xlabel(\"Answer\")\n",
    "        ax.legend(fontsize=8)\n",
    "    else:\n",
    "        ax.set_title(f\"{name}\\n(sin Judge eval)\")\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §10 — Mini-eval: N preguntas aleatorias\n",
    "\n",
    "Genera + evalúa con Judge una muestra aleatoria del holdout para **un checkpoint**.\n",
    "Útil para comparar métricas agregadas entre checkpoints sin correr las 500 preguntas completas.\n",
    "\n",
    "**Costo estimado**: `N × $0.009` por checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_CHECKPOINT = \"base_zeroshot\"   # ← checkpoint a evaluar\n",
    "N_QUESTIONS     = 20                # ← cantidad de preguntas (ajustar según presupuesto)\n",
    "EVAL_SEED       = 42\n",
    "\n",
    "rng     = random.Random(EVAL_SEED)\n",
    "sample  = rng.sample(holdout_with_scores, min(N_QUESTIONS, len(holdout_with_scores)))\n",
    "\n",
    "print(f\"Mini-eval: {len(sample)} preguntas con {EVAL_CHECKPOINT!r}\")\n",
    "print(f\"Costo estimado: ~${len(sample) * 0.009:.2f}\")\n",
    "\n",
    "\n",
    "async def run_mini_eval(checkpoint_name: str, entries: list) -> pd.DataFrame:\n",
    "    judge = Judge(model=JUDGE_MODEL, max_concurrent=20, max_cache_size=0)\n",
    "    rows  = []\n",
    "    for i, e in enumerate(entries):\n",
    "        print(f\"  [{i+1:3d}/{len(entries)}] {e.get('question_id', '?')[:40]}\", end=\"\\r\")\n",
    "        rubric = generate_rubric(checkpoint_name, e, temperature=TEMPERATURE)\n",
    "        scores = await judge.evaluate_answers_batched(\n",
    "            question=e[\"question\"],\n",
    "            answers=e[\"answers\"],\n",
    "            rubric=rubric,\n",
    "        )\n",
    "        rows.append({\n",
    "            \"question_id\"   : e.get(\"question_id\", e.get(\"prompt_id\", i)),\n",
    "            \"alignment\"     : alignment_score(scores, e[\"gold_scores\"]),\n",
    "            \"discrimination\": discrimination_score(scores),\n",
    "            \"info_value\"    : info_value(scores),\n",
    "            \"format\"        : format_validity(rubric),\n",
    "            \"points_sum\"    : points_sum(rubric),\n",
    "            \"n_items\"       : sum(1 for l in rubric.splitlines() if l.strip().startswith(\"Points:\")),\n",
    "            \"rubric_text\"   : rubric,   # guardamos el texto para inspección\n",
    "        })\n",
    "    print()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_mini = await run_mini_eval(EVAL_CHECKPOINT, sample)\n",
    "\n",
    "print(f\"\\nMini-eval completado — {len(df_mini)} preguntas, checkpoint: {EVAL_CHECKPOINT!r}\")\n",
    "print(\"\\nEstadísticas agregadas:\")\n",
    "display(df_mini[[\"alignment\", \"discrimination\", \"info_value\", \"format\", \"points_sum\"]].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Distribución de alignment scores ─────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(df_mini[\"alignment\"], bins=20, edgecolor=\"white\", color=\"steelblue\")\n",
    "axes[0].axvline(df_mini[\"alignment\"].mean(), color=\"red\", linestyle=\"--\", label=f\"mean={df_mini['alignment'].mean():.3f}\")\n",
    "axes[0].set_title(f\"Alignment (Spearman)\\n{EVAL_CHECKPOINT}\")\n",
    "axes[0].set_xlabel(\"Spearman correlation\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df_mini[\"discrimination\"], bins=20, edgecolor=\"white\", color=\"darkorange\")\n",
    "axes[1].axvline(df_mini[\"discrimination\"].mean(), color=\"red\", linestyle=\"--\",\n",
    "               label=f\"mean={df_mini['discrimination'].mean():.3f}\")\n",
    "axes[1].set_title(\"Discrimination (std de scores)\")\n",
    "axes[1].set_xlabel(\"Std\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(df_mini[\"format\"], bins=15, edgecolor=\"white\", color=\"green\")\n",
    "axes[2].axvline(df_mini[\"format\"].mean(), color=\"red\", linestyle=\"--\",\n",
    "               label=f\"mean={df_mini['format'].mean():.3f}\")\n",
    "axes[2].set_title(\"Format validity\")\n",
    "axes[2].set_xlabel(\"Fracción de líneas válidas\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Inspeccionar casos extremos ───────────────────────────────────────────────\n",
    "# Las mejores y peores rúbricas según alignment\n",
    "\n",
    "print(\"=== Top 3 mejores (mayor alignment) ===\")\n",
    "for _, row in df_mini.nlargest(3, \"alignment\").iterrows():\n",
    "    print(f\"\\nalignment={row['alignment']:.3f} | id={row['question_id']}\")\n",
    "    print(row[\"rubric_text\"][:400] + \"...\")\n",
    "\n",
    "print(\"\\n=== Bottom 3 peores (menor alignment) ===\")\n",
    "for _, row in df_mini.nsmallest(3, \"alignment\").iterrows():\n",
    "    print(f\"\\nalignment={row['alignment']:.3f} | id={row['question_id']}\")\n",
    "    print(row[\"rubric_text\"][:400] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Guardar resultados del mini-eval para comparar después ───────────────────\n",
    "RESULTS_DIR = ROOT / \"data\" / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "out_path = RESULTS_DIR / f\"mini_eval_{EVAL_CHECKPOINT}_n{len(df_mini)}.csv\"\n",
    "df_mini.to_csv(out_path, index=False)\n",
    "print(f\"Guardado en: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §11 — Comparar métricas entre checkpoints\n",
    "\n",
    "Si corriste mini-eval para varios checkpoints y guardaste los CSVs,\n",
    "esta celda los carga y los compara en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = ROOT / \"data\" / \"results\"\n",
    "\n",
    "# Cargar todos los mini-eval CSVs disponibles\n",
    "all_evals = {}\n",
    "for csv_path in sorted(RESULTS_DIR.glob(\"mini_eval_*.csv\")):\n",
    "    name = csv_path.stem.replace(\"mini_eval_\", \"\").rsplit(\"_n\", 1)[0]\n",
    "    df = pd.read_csv(csv_path)\n",
    "    all_evals[name] = df\n",
    "    print(f\"  {name}: {len(df)} preguntas\")\n",
    "\n",
    "if not all_evals:\n",
    "    print(\"No hay CSVs de mini-eval. Corré §10 primero.\")\n",
    "else:\n",
    "    metrics = [\"alignment\", \"discrimination\", \"info_value\", \"format\", \"points_sum\"]\n",
    "    comparison = pd.DataFrame({\n",
    "        name: df[metrics].mean()\n",
    "        for name, df in all_evals.items()\n",
    "    }).T.round(3)\n",
    "    print(\"\\nComparación de checkpoints (media sobre preguntas compartidas):\")\n",
    "    display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Referencia rápida\n",
    "\n",
    "| Métrica | Rango | Qué mide |\n",
    "|---------|-------|----------|\n",
    "| `alignment` | [-1, 1] | Spearman vs gold_scores. **Principal métrica del paper.** |\n",
    "| `discrimination` | [0, 1] | Std de los scores del Judge. 0 = rúbrica degenerada. |\n",
    "| `info_value` | [0, 1] | `4p(1-p)` donde p = frac. que pasa. Máx en p=0.5. |\n",
    "| `format_validity` | [0, 1] | Fracción de líneas con formato `Points: X, Item: Y`. |\n",
    "| `points_sum` | 0-∞ | Suma de todos los Points. Target: **10.0**. |\n",
    "\n",
    "**Comandos útiles:**\n",
    "```python\n",
    "unload_checkpoint(\"base_zeroshot\")  # liberar VRAM\n",
    "unload_all()                         # liberar todo\n",
    "list(_model_cache.keys())            # modelos en caché\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dug2p91pu7",
   "source": "## §12 — Rúbricas guardadas durante el training\n\nDurante el GRPO, la reward function guarda automáticamente el texto de cada rúbrica generada\nen `data/results/rubrics/step_XXXX.jsonl` (una línea JSON por rollout).\n\nCada entrada tiene: `step`, `question_id`, `question`, `rubric`, `alignment`, `reward`,\n`judge_scores`, `gold_scores`, `n_chars`.\n\nEsto permite ver cómo evolucionan las rúbricas **sin recargar checkpoints** — son las que el\nmodelo generó durante el training real (no regeneraciones a posteriori).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ilzncwvia6l",
   "source": "RUBRICS_DIR = ROOT / \"data\" / \"results\" / \"rubrics\"\n\n# ─── Cargar todos los step_XXXX.jsonl disponibles ─────────────────────────────\nstep_data = {}  # step_num → list of entries\n\nif not RUBRICS_DIR.exists():\n    print(f\"No hay rúbricas de training todavía ({RUBRICS_DIR})\")\n    print(\"Asegurate de que SAVE_RUBRICS no esté en 0 y de haber hecho al menos 1 step.\")\nelse:\n    for jl in sorted(RUBRICS_DIR.glob(\"step_*.jsonl\"),\n                     key=lambda p: int(p.stem.split(\"_\")[1])):\n        step_num = int(jl.stem.split(\"_\")[1])\n        with open(jl, encoding=\"utf-8\") as f:\n            entries = [json.loads(line) for line in f if line.strip()]\n        step_data[step_num] = entries\n        print(f\"  step {step_num:4d} — {len(entries):3d} rúbricas | \"\n              f\"alignment mean={np.mean([e['alignment'] for e in entries]):.3f} \"\n              f\"| reward mean={np.mean([e['reward'] for e in entries]):.3f}\")\n\n    print(f\"\\nTotal steps: {len(step_data)} | \"\n          f\"Total rúbricas: {sum(len(v) for v in step_data.values())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cilt2c7k98",
   "source": "# ─── Evolución de alignment y reward a lo largo del training ──────────────────\nif step_data:\n    steps       = sorted(step_data.keys())\n    mean_align  = [np.mean([e[\"alignment\"] for e in step_data[s]]) for s in steps]\n    mean_reward = [np.mean([e[\"reward\"]    for e in step_data[s]]) for s in steps]\n    mean_chars  = [np.mean([e[\"n_chars\"]   for e in step_data[s]]) for s in steps]\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n    axes[0].plot(steps, mean_align,  \"o-\", color=\"steelblue\")\n    axes[0].set_title(\"Alignment (Spearman) durante training\")\n    axes[0].set_xlabel(\"Step\")\n    axes[0].set_ylabel(\"Spearman correlation (media del batch)\")\n    axes[0].grid(alpha=0.3)\n\n    axes[1].plot(steps, mean_reward, \"o-\", color=\"darkorange\")\n    axes[1].set_title(\"Reward total durante training\")\n    axes[1].set_xlabel(\"Step\")\n    axes[1].set_ylabel(\"Reward (media del batch)\")\n    axes[1].grid(alpha=0.3)\n\n    axes[2].plot(steps, mean_chars,  \"o-\", color=\"green\")\n    axes[2].set_title(\"Largo de rúbricas durante training\")\n    axes[2].set_xlabel(\"Step\")\n    axes[2].set_ylabel(\"Chars (media del batch)\")\n    axes[2].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "28by3v2arcd",
   "source": "# ─── Inspeccionar rúbricas de un step específico ──────────────────────────────\nINSPECT_STEP = steps[0] if step_data else None  # ← cambiar al step que querés ver\nTOP_N        = 3   # mostrar las N de mayor y menor alignment\n\nif INSPECT_STEP and INSPECT_STEP in step_data:\n    entries_step = step_data[INSPECT_STEP]\n    print(f\"=== Step {INSPECT_STEP} — {len(entries_step)} rúbricas ===\")\n    print(f\"alignment: mean={np.mean([e['alignment'] for e in entries_step]):.3f} \"\n          f\"std={np.std([e['alignment'] for e in entries_step]):.3f}\")\n    print(f\"reward   : mean={np.mean([e['reward'] for e in entries_step]):.3f}\\n\")\n\n    sorted_entries = sorted(entries_step, key=lambda e: e[\"alignment\"], reverse=True)\n\n    print(f\"--- Top {TOP_N} (mayor alignment) ---\")\n    for e in sorted_entries[:TOP_N]:\n        print(f\"\\nalignment={e['alignment']:.3f} | reward={e['reward']:.3f} | chars={e['n_chars']}\")\n        print(f\"Q: {e['question'][:120]}...\")\n        print(f\"Rubric:\\n{e['rubric']}\")\n        print()\n\n    print(f\"\\n--- Bottom {TOP_N} (menor alignment) ---\")\n    for e in sorted_entries[-TOP_N:]:\n        print(f\"\\nalignment={e['alignment']:.3f} | reward={e['reward']:.3f} | chars={e['n_chars']}\")\n        print(f\"Q: {e['question'][:120]}...\")\n        print(f\"Rubric:\\n{e['rubric']}\")\n        print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (RL)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}